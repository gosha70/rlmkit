# RLMKit Environment Configuration
# Copy this file to .env and fill in your values

# ============================================================
# LLM Provider API Keys
# ============================================================
# RLMKit will auto-detect the first available provider.
# Uncomment and set the one(s) you want to use.

# OpenAI (https://platform.openai.com/api-keys)
# Supports: GPT-4, GPT-4 Turbo, GPT-3.5
OPENAI_API_KEY=sk-your-key-here

# Anthropic Claude (https://console.anthropic.com/)
# Supports: Claude 3 Opus, Sonnet, Haiku
# ANTHROPIC_API_KEY=sk-ant-your-key-here

# Ollama (Local LLM server)
# Default: http://localhost:11434
# Supports: Llama3, Mistral, Phi, and more
# Uncomment to override default:
# OLLAMA_BASE_URL=http://localhost:11434

# LM Studio (Local LLM server)
# Default port: 1234
# LMSTUDIO_BASE_URL=http://localhost:1234

# vLLM (High-performance inference server)
# VLLM_BASE_URL=http://localhost:8000

# ============================================================
# Optional: Default Model Selection
# ============================================================
# Override the default model for each provider

# RLMKIT_OPENAI_DEFAULT_MODEL=gpt-4o-mini
# RLMKIT_ANTHROPIC_DEFAULT_MODEL=claude-3-5-sonnet-20241022
# RLMKIT_OLLAMA_DEFAULT_MODEL=llama3.2

# ============================================================
# Optional: Execution Configuration
# ============================================================

# Maximum execution steps for RLM mode
# RLMKIT_MAX_STEPS=50

# Timeout for code execution (seconds)
# RLMKIT_TIMEOUT=30

# Enable verbose logging
# RLMKIT_VERBOSE=false

# ============================================================
# Usage Examples
# ============================================================
#
# 1. Auto-detection (uses first available provider):
#    from rlmkit import interact
#    result = interact("content", "query")
#
# 2. Explicit provider:
#    result = interact("content", "query", provider="openai")
#
# 3. Check available providers:
#    from rlmkit.llm import list_available_providers
#    print(list_available_providers())
#
# 4. Get provider info:
#    from rlmkit.llm import get_provider_info
#    print(get_provider_info())
