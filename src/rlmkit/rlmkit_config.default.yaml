# RLMKit Configuration Example
# Copy this file to one of the following locations and customize:
#   - ./rlmkit_config.yaml (project directory)
#   - ~/.rlmkit/config.yaml (user home)
#   - /etc/rlmkit/config.yaml (system-wide)
# Or set RLMKIT_CONFIG_PATH environment variable

# Security Configuration
security:
  # Modules that are always blocked (high security risk)
  blocked_modules:
    # System operations
    - os
    - sys
    - subprocess
    - socket
    - socketserver
    
    # Network operations
    - http
    - urllib
    - ftplib
    - telnetlib
    - smtplib
    
    # File system operations
    - pathlib
    - shutil
    - tempfile
    - glob
    - fnmatch
    
    # Low-level system access
    - pty
    - tty
    - pipes
    - resource
    - syslog
    - ctypes
    - cffi
    - mmap
    - signal
    - fcntl
    
    # Serialization (security risk)
    - pickle
    - shelve
    - dbm
    - sqlite3
    
    # Import manipulation
    - importlib
    - pkgutil
    - modulefinder
    - runpy
    - __builtin__
    - __builtins__
    - builtins
  
  # Modules that are allowed in safe mode
  safe_modules:
    # Data processing
    - json
    - re
    - math
    - datetime
    - time
    - calendar
    - decimal
    - fractions
    - statistics
    - random
    
    # String operations
    - string
    - textwrap
    - unicodedata
    
    # Functional programming
    - itertools
    - functools
    - operator
    - collections
    - heapq
    - bisect
    - array
    - copy
    - pprint
    
    # Type utilities
    - enum
    - dataclasses
    - typing
  
  # Builtins that are blocked in safe mode
  blocked_builtins:
    - open
    - input
    - compile
    - eval
    - exec
    - __import__
    - breakpoint
    - exit
    - quit
    - help
  
  # Builtins that are allowed in safe mode
  safe_builtins:
    # Basic functions
    - abs
    - all
    - any
    - ascii
    - bin
    - bool
    - bytearray
    - bytes
    - callable
    - chr
    - classmethod
    - complex
    - delattr
    - dict
    - dir
    - divmod
    - enumerate
    - filter
    - float
    - format
    - frozenset
    - getattr
    - hasattr
    - hash
    - hex
    - id
    - int
    - isinstance
    - issubclass
    - iter
    - len
    - list
    - locals
    - map
    - max
    - min
    - next
    - object
    - oct
    - ord
    - pow
    - print
    - property
    - range
    - repr
    - reversed
    - round
    - set
    - setattr
    - slice
    - sorted
    - staticmethod
    - str
    - sum
    - super
    - tuple
    - type
    - vars
    - zip
    
    # Exceptions
    - Exception
    - ValueError
    - TypeError
    - KeyError
    - IndexError
    - AttributeError
    - RuntimeError
    - StopIteration
    - AssertionError
    
    # Constants
    - "True"
    - "False"
    - "None"
    - NotImplemented
    - Ellipsis
    - __name__
    - __doc__
    - __package__
    - __loader__
    - __spec__

# Execution Configuration
execution:
  # Default timeout for code execution (in seconds)
  default_timeout: 5.0
  
  # Maximum characters in stdout/stderr output
  max_output_chars: 10000
  
  # Default safe mode setting
  default_safe_mode: false

# Monitoring Configuration
monitoring:
  # Enable telemetry collection
  enable_telemetry: false
  
  # Log level (DEBUG, INFO, WARNING, ERROR)
  log_level: INFO
  
  # Export format for telemetry (json, csv, prometheus)
  export_format: json

# LLM Configuration
llm:
  # Default provider to use (if not specified programmatically)
  # Comment out or remove to require explicit provider in code
  default_provider:
    provider: openai  # Options: openai, anthropic, ollama, vllm
    model: gpt-4
    # api_key: null  # If null, will use environment variable (OPENAI_API_KEY, etc.)
    # base_url: null  # Custom API endpoint (optional)
    temperature: 0.7
    # max_tokens: null  # Maximum tokens to generate (optional, required for Claude)
    # organization: null  # OpenAI organization ID (optional)
  
  # Model pricing database (cost per 1 million tokens in USD)
  # Used for budget tracking and cost estimation
  # Update these values as pricing changes
  pricing:
    # OpenAI Models
    gpt-4:
      input_cost_per_1m: 30.0
      output_cost_per_1m: 60.0
    
    gpt-4-turbo:
      input_cost_per_1m: 10.0
      output_cost_per_1m: 30.0
    
    gpt-4-turbo-preview:
      input_cost_per_1m: 10.0
      output_cost_per_1m: 30.0
    
    gpt-3.5-turbo:
      input_cost_per_1m: 0.50
      output_cost_per_1m: 1.50
    
    gpt-3.5-turbo-16k:
      input_cost_per_1m: 3.0
      output_cost_per_1m: 4.0
    
    # Anthropic Claude Models
    claude-3-opus-20240229:
      input_cost_per_1m: 15.0
      output_cost_per_1m: 75.0
    
    claude-3-sonnet-20240229:
      input_cost_per_1m: 3.0
      output_cost_per_1m: 15.0
    
    claude-3-haiku-20240307:
      input_cost_per_1m: 0.25
      output_cost_per_1m: 1.25
    
    claude-2.1:
      input_cost_per_1m: 8.0
      output_cost_per_1m: 24.0
    
    claude-2.0:
      input_cost_per_1m: 8.0
      output_cost_per_1m: 24.0
    
    # Local models (Ollama, vLLM) typically have zero cost
    # But you can add entries if running on paid cloud infrastructure
    llama2:
      input_cost_per_1m: 0.0
      output_cost_per_1m: 0.0
    
    mistral:
      input_cost_per_1m: 0.0
      output_cost_per_1m: 0.0
